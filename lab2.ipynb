{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Лабораторная работа №2 по курсу \"Машинное обучение\"</h1>\n",
    "<p style=\"text-align:right; font-size:125%\"><i>Выполнил студент группы М8О-307Б Лопатин Александр</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать алгоритмы машинного обучения. Применить данные\n",
    "алгоритмы на наборы данных, подготовленных в первой лабораторной работе.\n",
    "Провести анализ полученных моделей, вычислить метрики классификатора.\n",
    "Произвести тюнинг параметров в случае необходимости. Сравнить полученные\n",
    "результаты с моделями реализованными в scikit-learn. Аналогично построить метрики\n",
    "классификации. Показать, что полученные модели не переобучились. Также необходимо\n",
    "сделать выводы о применимости данных моделей к вашей задаче.\n",
    "\n",
    "1. ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ +\n",
    "2. KNN +\n",
    "3. SVM -\n",
    "4. ДЕРЕВО РЕШЕНИЙ +\n",
    "5. RANDOM FOREST +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По заданным признакам страны (год, количество населения, ВВП на душу населения, количество самоубийств на 100 тыс. населения) определить, является ли эта страна развивающейся или развитой. Для определения этого критерия можно взять сравнение индекса человеческого развития с его медианным значением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"datasets/\"\n",
    "def loadDataSet(name):\n",
    "    csv_path = os.path.join(directory, name)   \n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = loadDataSet(\"first.csv\")\n",
    "total = DS.groupby([\"country-year\"]).agg({\"country-year\": \"first\", \"suicides_no\": \"sum\",\"population\": \"sum\"})\n",
    "total[\"suicides_no\"] = 100000 * total[\"suicides_no\"]/total[\"population\"]\n",
    "total.rename(columns={'suicides_no':'total suicides/100k pop'}, inplace=True)\n",
    "del total[\"population\"]\n",
    "DS = DS.join(total.set_index('country-year'), on='country-year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на тестовую и обучающую выборки и вычислим для них метки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "clf = LogisticRegression()\n",
    "X = DS.groupby([\"country-year\"]).agg({\"total suicides/100k pop\": \"first\",\n",
    "                                           \"population\": \"sum\", \n",
    "                                              \"year\": \"first\",\n",
    "                                           \"HDI for year\": \"first\",\n",
    "                                          \"gdp_per_capita ($)\": \"first\"\n",
    "                                          })\n",
    "\n",
    "median = (X[\"HDI for year\"]).median()\n",
    "Y = (X[\"HDI for year\"] <= median).astype(int)\n",
    "\n",
    "del X[\"HDI for year\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, Y,stratify=Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия — метод построения линейного классификатора, позволяющий оценивать апостериорные вероятности принадлежности объектов классам. Для оптимизации был выбран метод градиентного спуска с постоянным шагом. Так как отношение классов далеко от единицы, метрика accuracy даст мало информации (поскольку функцией рандома можно получить accuracy > 0.86), поэтому в качестве метрики в этом и последующих методах будет выступать precision. Для average выбран параметр weighted для учёта дисбаланса меток.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mins = np.min(X, axis = 0) \n",
    "    maxs = np.max(X, axis = 0) \n",
    "    rng = maxs - mins \n",
    "    norm_X = 1 - ((maxs - X)/rng) \n",
    "    return norm_X\n",
    "def logistic_func(beta, X):\n",
    "    return 1.0/(1 + np.exp(-np.dot(X, beta.T))) \n",
    "\n",
    "def log_gradient(beta, X, y):\n",
    "    first_calc = logistic_func(beta, X) - y.reshape(X.shape[0], -1) \n",
    "    final_calc = np.dot(first_calc.T, X) \n",
    "    return final_calc \n",
    "\n",
    "def cost_func(beta, X, y): \n",
    "    log_func_v = logistic_func(beta, X) \n",
    "    y = np.squeeze(y) \n",
    "    step1 = y * np.log(log_func_v) \n",
    "    step2 = (1 - y) * np.log(1 - log_func_v) \n",
    "    final = -step1 - step2 \n",
    "    return np.mean(final) \n",
    "\n",
    "def grad_desc(X, y, beta, lr=.01, converge_change=.001): \n",
    "    cost = cost_func(beta, X, y) \n",
    "    change_cost = 1\n",
    "    while(change_cost > converge_change): \n",
    "        old_cost = cost \n",
    "        beta = beta - (lr * log_gradient(beta, X, y)) \n",
    "        cost = cost_func(beta, X, y) \n",
    "        change_cost = old_cost - cost\n",
    "    return beta\n",
    "\n",
    "def pred_values(beta, X): \n",
    "    pred_prob = logistic_func(beta, X) \n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0) \n",
    "    return np.squeeze(pred_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат работы метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 0.7567051059078385\n",
      "test precision: 0.7531059408177094\n"
     ]
    }
   ],
   "source": [
    "X = normalize(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, Y,stratify=Y, test_size=0.1)\n",
    "beta = np.matrix(np.zeros(X.shape[1]))\n",
    "beta = grad_desc(X_train.values, y_train.values, beta)\n",
    "\n",
    "print(\"train precision: \" + str(precision_score(y_train, pred_values(beta, X_train), average='weighted')))\n",
    "print(\"test precision: \" + str(precision_score(y_test, pred_values(beta, X_test), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат работы метода из scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 0.721035225921522\n",
      "test precision: 0.7221352391828916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"train precision: \" + str(precision_score(y_train, clf.predict(X_train), average='weighted')))\n",
    "print(\"test precision: \" + str(precision_score(y_test, clf.predict(X_test), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика на тренировочных данных чуть выше, чем на тестовых, что может свидетельствовать о переобучении. Результаты работы метода scikit-learn чуть хуже результатов моей реализации. В целом, данный метод дал неудовлетворительные результаты в обоих случаях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод k-ближайших соседей (KNN) — метрический алгоритм для автоматической классификации объектов или регрессии. В качестве расстояния было взято расстояние в евклидовом пространстве.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SquareEuclidDistance(a,b):\n",
    "    d = 0\n",
    "    for i in range(a.shape[0]):\n",
    "        d += (a[i] - b[i]) * (a[i] - b[i])\n",
    "    return d\n",
    "\n",
    "def KNN(X_train, Y_train, X_test):\n",
    "    Y_test = np.ones(X_test.shape[0])\n",
    "    for j in range(X_test.shape[0]):\n",
    "        Q = np.zeros(Y_train.max() + 1)\n",
    "        for i in range(X_train.shape[0]):\n",
    "            Q[Y_train[i]] += 1/SquareEuclidDistance(X_test[j,:], X_train[i,:])\n",
    "        Y_test[j] = np.argmax(Q)\n",
    "    return Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат работы метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-263-9bd7eaa9fbe9>:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  Q[Y_train[i]] += 1/SquareEuclidDistance(X_test[j,:], X_train[i,:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 1.0\n",
      "test precision: 0.904732577609269\n"
     ]
    }
   ],
   "source": [
    "print(\"train precision: \" + str(precision_score(y_test, KNN(X_test.values, y_test.values, X_test.values), average='weighted')))\n",
    "print(\"test precision: \" + str(precision_score(y_test, KNN(X_train.values, y_train.values, X_test.values), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат работы метода из scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 0.9267362127510098\n",
      "test precision: 0.9037564413567399\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"train precision: \" + str(precision_score(y_train, clf.predict(X_train), average='weighted')))\n",
    "print(\"test precision: \" + str(precision_score(y_test, clf.predict(X_test), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих случаях заметно, что модель переобучилась. Метод из scikit-learn дал чуть большую точность. Для данного датасета этот метод показывает себя лучше, чем метод логистической регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дерево решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Деревья решений являются одним из наиболее эффективных инструментов интеллектуального анализа данных и предсказательной аналитики, которые позволяют решать задачи классификации и регрессии. Для построения дерева решений использовался алгоритм CART.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_prun = 0.1\n",
    "\n",
    "    def fit(self, X, y, random_feature = False):\n",
    "        self.n_classes_ = len(set(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y, random_feature)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "\n",
    "    def _best_split(self, X, y,random_feature):\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "        for idx in range(self.n_features_):\n",
    "            if(np.random.randint(0, 11) <= self.feature_prun*10):\n",
    "                continue\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            num_left = [0] * self.n_classes_\n",
    "            num_right = num_parent.copy()\n",
    "            for i in range(1, m):\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _grow_tree(self, X, y,random_feature, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y,random_feature)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left,random_feature, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right,random_feature, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат работы метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 0.976749122348028\n",
      "test precision: 0.9533960770644067\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=10)\n",
    "clf.fit(X_train.values, y_train.values)\n",
    "print(\"train precision: \" + str(precision_score(y_train, clf.predict(X_train.values), average='weighted')))\n",
    "print(\"test precision: \" + str(precision_score(y_test, clf.predict(X_test.values), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат работы метода из scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 1.0\n",
      "test precision: 0.9666938112676923\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(\"train precision: \" + str(precision_score(y_train, clf.predict(X_train), average='weighted')))\n",
    "print(\"test precision: \" + str(precision_score(y_test, clf.predict(X_test), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты работы моей реализации и реализации scikit-learn примерно совпадают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest - модель, состоящая из множества деревьев решений. Вместо того,чтобы просто усреднять прогнозы разных деревьев (такая концепция называется просто «лес»), эта модель использует две ключевые концепции, которые и делают этот лес случайным:\n",
    "1. Случайная выборка образцов из набора данных при построении деревьев.\n",
    "2. При разделении узлов выбираются случайные наборы параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(size, Max_depth):\n",
    "    head = [None] * size\n",
    "    for i in range(size):\n",
    "        head[i] = DecisionTreeClassifier(max_depth=Max_depth)\n",
    "    return head\n",
    "def fit(forest, X_train, y_train):\n",
    "    for i in range(len(forest)):\n",
    "        subset = np.zeros(X_train.shape)\n",
    "        labels = np.zeros(y_train.shape).astype(int)\n",
    "        for j in range(X_train.shape[0]):\n",
    "            index = np.random.randint(0, X_train.shape[0])\n",
    "            subset[j] = X_train.values[index]\n",
    "            labels[j] = y_train.values[index]\n",
    "        forest[i].fit(X_train.values, y_train.values,random_feature = True)\n",
    "def predict(forest, X):\n",
    "    Q = np.zeros([X.shape[0], 2])\n",
    "    for i in range(len(forest)):\n",
    "        pred = forest[i].predict(X.values)\n",
    "        for j in range(len(pred)):\n",
    "            Q[j, pred[j]] += 1\n",
    "    pred = np.zeros([X.shape[0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        pred[i] = np.argmax(Q[i,:])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат работы метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 0.9208401027316468\n",
      "test precision: 0.9139748512913184\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForest(10, 4)\n",
    "fit(clf, X_train, y_train)\n",
    "print(\"train precision: \" + str(precision_score(y_train, predict(clf,X_train), average='weighted')))\n",
    "print(\"test precision: \" + str(precision_score(y_test, predict(clf,X_test), average='weighted')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат работы метода из scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 0.9219551977156076\n",
      "test precision: 0.9197078346173327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=4)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"train precision: \" + str(precision_score(y_train, clf.predict(X_train), average='weighted')))\n",
    "print(\"test precision: \" + str(precision_score(y_test, clf.predict(X_test), average='weighted')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моя реализация метода и реализация метода scikit-learn показывают примерно одни и те же результаты. В целом, на данном датасете метод random forest чуть менее точен, чем просто одно дерево решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе данной лабораторной работы были детально изучены простейшие алгоритмы машинного обучения, было проведено сравнение их работы с работой тех же алгоритмов в библиотеке scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
